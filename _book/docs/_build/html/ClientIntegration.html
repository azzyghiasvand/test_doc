
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Cerebro Data Access Service Integration &#8212; Cerebro Data 1.0 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="cerebro-data-access-service-integration">
<span id="cerebro-data-access-service-integration"></span><h1>Cerebro Data Access Service Integration<a class="headerlink" href="#cerebro-data-access-service-integration" title="Permalink to this headline">¶</a></h1>
<p>This documents describes how to use Cerebro Data Access Service (CDAS) from a user’s
perspective. It describes how to configure those tools to use Cerebro.</p>
<div class="section" id="hadoop-ecosystem-tools-spark-mapreduce-hive-presto">
<span id="hadoop-ecosystem-tools-spark-mapreduce-hive-presto"></span><h2>Hadoop ecosystem tools (spark, mapreduce, hive, presto)<a class="headerlink" href="#hadoop-ecosystem-tools-spark-mapreduce-hive-presto" title="Permalink to this headline">¶</a></h2>
<div class="section" id="client-libraries">
<span id="client-libraries"></span><h3>Client libraries<a class="headerlink" href="#client-libraries" title="Permalink to this headline">¶</a></h3>
<p>For all of the Hadoop ecosystem tools, it is required to include the Cerebro client
libraries. These libraries leverage the analytic tool’s pluggable interfaces to
communicate and handle the communication and data exchange with the Cerebro servers.
Depending on the analytics tool, this library can be provided in different ways. For
example, it can be installed on a system wide class path, can be provided at the time
the job is submitted (e.g. <code class="docutils literal notranslate"><span class="pre">spark</span> <span class="pre">submit</span> <span class="pre">--jars</span></code>) or bundled into the application (e.g.
by including it with Maven).</p>
</div>
<div class="section" id="configurations">
<span id="configurations"></span><h3>Configurations<a class="headerlink" href="#configurations" title="Permalink to this headline">¶</a></h3>
<p>In addition to the library, we require a couple of configs to be set. We also expose
various optional configs to fine tune the system behavior.</p>
<p>Note: when the configs are set in spark, they are prefixed with <code class="docutils literal notranslate"><span class="pre">spark.</span></code>. For example,
the config <code class="docutils literal notranslate"><span class="pre">recordservice.kerberos.principal</span></code>, when configured for spark, should
be <code class="docutils literal notranslate"><span class="pre">spark.recordservice.kerberos.principal.</span></code> This is true for all configs.</p>
<div class="section" id="required">
<span id="required"></span><h4>Required<a class="headerlink" href="#required" title="Permalink to this headline">¶</a></h4>
<p><strong>recordservice.planner.hostports</strong></p>
<p>This is always required and is a comma separated list of host:ports where the CDAS
planners are running.</p>
</div>
<div class="section" id="authentication-configs">
<span id="authentication-configs"></span><h4>Authentication configs<a class="headerlink" href="#authentication-configs" title="Permalink to this headline">¶</a></h4>
<p><strong>recordservice.kerberos.principal</strong></p>
<p>This is the principal of the planner service to connect to. This is a 3 part
principal: <code class="docutils literal notranslate"><span class="pre">SERVICE_NAME/SERVICE_HOST&#64;REALM</span></code>, for example,
<code class="docutils literal notranslate"><span class="pre">cerebro/planner.cerebro.com&#64;CEREBRO.COM</span></code>. This is required if the client is
authenticating with CDAS using kerberos.</p>
<p><strong>recordservice.delegation-token.token</strong></p>
<p>This is the token string for this user. CDAS can be configured to accept multiple
kinds of tokens but it is the same config for clients.</p>
<p>Note: If both token and principal are specified, the client will only authenticate
using the token.</p>
</div>
</div>
</div>
<div class="section" id="tool-integration">
<span id="tool-integration"></span><h2>Tool integration<a class="headerlink" href="#tool-integration" title="Permalink to this headline">¶</a></h2>
<div class="section" id="hiveserver2-beeline-hive-cli">
<span id="hiveserver2-beeline-hive-cli"></span><h3>HiveServer2/Beeline/hive cli<a class="headerlink" href="#hiveserver2-beeline-hive-cli" title="Permalink to this headline">¶</a></h3>
<p>HiveServer2 provides a service to run SQL.</p>
<p>The original hive shell (‘hive’) is also supported.</p>
<p>From the user’s point of view, they simply connect to HS2 as always. HS2 in fact is not
provided by Cerebro and clients talk to the same HS2 without directly interacting with
Cerebro (HS2 is configured and integrated with Cerebro). Authentication works exactly
as always.</p>
<div class="section" id="setting-up-the-admin-role-quick-start">
<span id="setting-up-the-admin-role-quick-start"></span><h4>Setting up the admin role quick start<a class="headerlink" href="#setting-up-the-admin-role-quick-start" title="Permalink to this headline">¶</a></h4>
<p>These are quick start steps to set up the admin role which has full access to the
server. The user running these commands needs to have admin to the Cerebro Catalog.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>beeline&gt; !connect jdbc:hive2://&lt;host:port of hs2&gt;/default<span class="p">;</span><span class="nv">principal</span><span class="o">=</span>&lt;hs2_principal&gt;
beeline&gt; CREATE ROLE admin_role<span class="p">;</span>
beeline&gt; GRANT ALL ON SERVER server1 TO ROLE admin_role<span class="p">;</span>
beeline&gt; GRANT ROLE admin_role TO GROUP &lt;YOUR ADMIN USER/GROUP&gt;<span class="p">;</span>
</pre></div>
</div>
<p><strong>Note</strong>: These steps assume a few things about your set up that are no different than
typical HS2 requirements. The admin user or group that is granted must exist on the unix
system in both Cerebro and HS2.</p>
</div>
<div class="section" id="creating-a-dataset">
<span id="creating-a-dataset"></span><h4>Creating a dataset<a class="headerlink" href="#creating-a-dataset" title="Permalink to this headline">¶</a></h4>
<p>In the next step, we will create an external dataset for data in S3.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">beeline</span><span class="o">&gt;</span> <span class="n">create</span> <span class="n">external</span> <span class="n">table</span> <span class="n">sample</span> <span class="p">(</span><span class="n">s</span> <span class="n">STRING</span><span class="p">)</span> <span class="n">LOCATION</span> <span class="s1">&#39;s3n://cerebrodata/sample&#39;</span>
<span class="n">beeline</span><span class="o">&gt;</span> <span class="n">show</span> <span class="n">tables</span><span class="p">;</span>
<span class="n">beeline</span><span class="o">&gt;</span> <span class="n">select</span> <span class="o">*</span> <span class="kn">from</span> <span class="nn">sample</span><span class="p">;</span>
</pre></div>
</div>
<p>At this point we have added a dataset to Cerebro By default only the admin user/group has
access to the dataset, which is now accessible to all the Cerebro integrated clients.
Other users accessing this dataset should see the request fail.</p>
<p><strong>Note:</strong> These steps also assumes that the beeline client has access to this location.
This, for example, involves IAM roles or AWS access keys to be set up if the data is in
S3.</p>
<p><strong>Note:</strong> Creating non-external table is currently considered undefined behavior and
should not be done.</p>
</div>
<div class="section" id="creating-a-view-and-granting-access-to-another-role">
<span id="creating-a-view-and-granting-access-to-another-role"></span><h4>Creating a view and granting access to another role<a class="headerlink" href="#creating-a-view-and-granting-access-to-another-role" title="Permalink to this headline">¶</a></h4>
<p>Finally, we will create a view and grant access to the view to a different set of
users. In this case we will create a view that only returns records which contain <code class="docutils literal notranslate"><span class="pre">test</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>beeline&gt; CREATE ROLE test_role<span class="p">;</span>
beeline&gt; GRANT ROLE test_role TO GROUP &lt;YOUR TEST USER/GROUP&gt;<span class="p">;</span>

beeline&gt; CREATE VIEW sample_view as SELECT * FROM sample WHERE s LIKE <span class="s1">&#39;%test%&#39;</span><span class="p">;</span>
beeline&gt; SHOW TABLES<span class="p">;</span>
beeline&gt; SELECT * FROM sample_view<span class="p">;</span>

beeline&gt; GRANT SELECT ON TABLE sample_view TO ROLE test_role<span class="p">;</span>
</pre></div>
</div>
<p>At this point the admin group should see the full dataset and the test group should only
see a subset of the records.</p>
<p>The remaining GRANT/REVOKE/DROP are supported and work identically to HS2.</p>
<p><strong>Note</strong>: Updating permissions can take a few minutes to be reflected everywhere in the
system as policies are cached.</p>
</div>
</div>
<div class="section" id="mapreduce-integration">
<span id="mapreduce-integration"></span><h3>MapReduce Integration<a class="headerlink" href="#mapreduce-integration" title="Permalink to this headline">¶</a></h3>
<p>The MapReduce integration is API compatible with the Cloudera open source
<a class="reference external" href="http://recordservice.io/examples/">RecordServiceClient</a>. For details on those APIs,
refer to those docs.</p>
<p>Running the MapReduce application is done as normal running;</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>hadoop jar &lt;application.jar&gt; <span class="o">[</span>arguments<span class="o">]</span>
</pre></div>
</div>
<div class="section" id="configuration">
<span id="configuration"></span><h4>Configuration<a class="headerlink" href="#configuration" title="Permalink to this headline">¶</a></h4>
<p>The only required configuration is the location of the RecordService planner port. This
can be configured either in the standard Hadoop config files
(<code class="docutils literal notranslate"><span class="pre">mapred-site.xml</span></code> or <code class="docutils literal notranslate"><span class="pre">yarn-site.xml</span></code>) or from the environment
(<code class="docutils literal notranslate"><span class="pre">RECORD_SERVICE_PLANNER_HOST</span></code>). In the config file, the name of the config is
<code class="docutils literal notranslate"><span class="pre">recordservice.planner.hostports</span></code>. In either case, the value should be the host port of
the planner. If both are set, the file based config takes precedence.</p>
<p>In a typical end user use case, the config should have been populated by the cluster
admin, typically in <code class="docutils literal notranslate"><span class="pre">/etc/hadoop/conf</span></code>. In this case, there is no configuration required
by the end user. If the config is not set, the client will by default connect to
localhost, likely resulting in connection errors.</p>
</div>
<div class="section" id="dependency-management">
<span id="dependency-management"></span><h4>Dependency management<a class="headerlink" href="#dependency-management" title="Permalink to this headline">¶</a></h4>
<p>Libraries (jars) that your application depend on need to be available on all the nodes
in the compute cluster. There are multiple resources on how to do this for Hadoop and
we will summarize here. There are two ways to do this:</p>
<ol class="simple">
<li>Create a fat jar/shade the dependencies in your application. This means that when
building the MapReduce application, you bundle all the dependencies and the application
is self contained. An example is how the RecordService examples do this with <a class="reference external" href="https://github.com/cloudera/RecordServiceClient/blob/master/java/examples/pom.xml#L92">maven</a>.</li>
<li>Provide all the dependent jars when submitting the job with <code class="docutils literal notranslate"><span class="pre">hadoop</span> <span class="pre">jar</span></code>. This requires
setting <code class="docutils literal notranslate"><span class="pre">HADOOP_CLASSPATH</span></code> to the jars (either folder or individual paths) and specifying
<code class="docutils literal notranslate"><span class="pre">--libjars</span></code> and passing all the dependencies. Note that <code class="docutils literal notranslate"><span class="pre">HADOOP_CLASSPATH</span></code> is colon
separated and <code class="docutils literal notranslate"><span class="pre">libjars</span></code> is comma separated.</li>
</ol>
<p>As a complete example, assuming we are using the <code class="docutils literal notranslate"><span class="pre">RecordServiceAvro</span></code> client library.
For option 1, we can simply just run the application as the dependencies have been
handled as part of the build.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>hadoop jar AvroApplication.jar &lt;arguments&gt;
</pre></div>
</div>
<p>For option 2, we need to:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># All jars are paths to the jar on the local (submitting machine) filesystem.</span>
<span class="nb">export</span> <span class="nv">HADOOP_CLASSPATH</span><span class="o">=</span>&lt;recordservice-avro.jar&gt;:&lt;recordservice-avro-mr.jar&gt;:&lt;recordservice-core.jar&gt;:&lt;recordservice-mr.jar&gt;
hadoop jar --libjars &lt;recordservice-avro.jar&gt;,&lt;recordservice-avro-mr.jar&gt;,&lt;recordservice-core.jar&gt;,&lt;recordservice-mr.jar&gt; AvroApplication.jar &lt;arguments&gt;
</pre></div>
</div>
</div>
</div>
<div class="section" id="pig-integration">
<span id="pig-integration"></span><h3>Pig Integration<a class="headerlink" href="#pig-integration" title="Permalink to this headline">¶</a></h3>
<p>The Pig Integration is very similar to the MapReduce integration. The API is also
compatible with the open source client. Pig handles some of the dependency management
automatically and the only required dependency jars is the
<code class="docutils literal notranslate"><span class="pre">recordservice-hcatalog-pid-adapter</span></code> jar.</p>
<div class="section" id="configuration">
<span id="id1"></span><h4>Configuration<a class="headerlink" href="#configuration" title="Permalink to this headline">¶</a></h4>
<p>Pig configuration is identical to MapReduce. Refer to that section.</p>
</div>
</div>
<div class="section" id="hue-integration">
<span id="hue-integration"></span><h3>Hue Integration<a class="headerlink" href="#hue-integration" title="Permalink to this headline">¶</a></h3>
<p>Hue does not require any additional steps to work with Cerebro. Hue also connects to
HiveServer2 and is integrated very similarly as beeline.</p>
<div class="section" id="troubleshooting">
<span id="troubleshooting"></span><h4>Troubleshooting<a class="headerlink" href="#troubleshooting" title="Permalink to this headline">¶</a></h4>
<p>If requests are failing with users not having privileges, ensure that the user exists.
The user must exist on (as a unix user):</p>
<ul class="simple">
<li>Machine running HS2.</li>
<li>Cerebro catalog.</li>
</ul>
</div>
</div>
<div class="section" id="impala-integration">
<span id="impala-integration"></span><h3>Impala Integration<a class="headerlink" href="#impala-integration" title="Permalink to this headline">¶</a></h3>
<p>Impala does not require any end user steps. After Impala is configured by the cluster
admin, users can connect to Impala via the shell as normal.</p>
</div>
<div class="section" id="spark-integration">
<span id="spark-integration"></span><h3>Spark Integration<a class="headerlink" href="#spark-integration" title="Permalink to this headline">¶</a></h3>
<p>Spark provides a few ways to integrate with spark. Refer to the open source client
documentation for <a class="reference external" href="https://github.com/cloudera/RecordServiceClient/tree/master/java/examples-spark">details</a></p>
<div class="section" id="hivecontext">
<span id="hivecontext"></span><h4>HiveContext<a class="headerlink" href="#hivecontext" title="Permalink to this headline">¶</a></h4>
<p>Spark can be configured to use an existing HiveMetastore to retrieve catalog metadata.
If this is set up, you can retrieve metadata as with a typical Hive client (e.g. s
imilar to beeline). If configured, you can issues queries such as</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sqlContext</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SHOW TABLES&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p><strong>Note</strong>: The sql supported is not necessarily compatible with beeline. This is more
intended to retrieve catalog metadata and not recommended as a way to administrate
access control policies (grant/revoke).</p>
</div>
<div class="section" id="configuration">
<span id="id2"></span><h4>Configuration<a class="headerlink" href="#configuration" title="Permalink to this headline">¶</a></h4>
<p>Spark requires a single configuration that is the host port of the planner:
<code class="docutils literal notranslate"><span class="pre">spark.recordservice.planner.hostports</span></code>. This should be set to a list of comma
separated host:port where the Cerebro planners are running. This configuration can
either be set system wide (typically <code class="docutils literal notranslate"><span class="pre">/etc/spark/conf/spark-defaults.conf</span></code>) or can be
specified when launching <code class="docutils literal notranslate"><span class="pre">spark-shell</span></code> or <code class="docutils literal notranslate"><span class="pre">spark-submit</span></code>. For example, you can connect
to a particular planner with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">spark</span><span class="o">-</span><span class="n">shell</span> <span class="o">--</span><span class="n">conf</span> <span class="n">spark</span><span class="o">.</span><span class="n">recordservice</span><span class="o">.</span><span class="n">planner</span><span class="o">.</span><span class="n">hostports</span><span class="o">=</span><span class="n">IP</span><span class="p">:</span><span class="n">PORT</span>
<span class="c1"># or</span>
<span class="n">spark</span><span class="o">-</span><span class="n">submit</span> <span class="o">--</span><span class="n">conf</span> <span class="n">spark</span><span class="o">.</span><span class="n">recordservice</span><span class="o">.</span><span class="n">hostports</span><span class="o">=</span><span class="n">IP</span><span class="p">:</span><span class="n">PORT</span> <span class="o">&lt;</span><span class="n">rest</span> <span class="n">of</span> <span class="n">args</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>If using the HiveContext to directly interact with HiveMetastore, the Cerebro catalog
configs will need to be set. This requires a configured hive client which is by default
located in <code class="docutils literal notranslate"><span class="pre">/etc/hive/conf/hive-site.xml</span></code>. The configuration should be configured by
the cluster admin as it can require various settings, particularly if kerberos is
enabled. To spot check, the config <code class="docutils literal notranslate"><span class="pre">hive.metastore.uris</span></code> should be set to the Cerebro
catalog.</p>
</div>
</div>
<div class="section" id="rest-scan-api">
<span id="rest-scan-api"></span><h3>REST Scan API<a class="headerlink" href="#rest-scan-api" title="Permalink to this headline">¶</a></h3>
<p>CDAS exposes a REST API that returns data as JSON. This API is only intended to read
data, not to register datasets or update their policies.</p>
<p>The REST API simply exposes a HTTP endpoint. This endpoint is referenced in other
documents as the <em>CDAS REST server</em> endpoint. To read data, you can simply reach:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>http://&lt;hostport&gt;/api/scan/&lt;dataset name&gt;
# You can optionally specify how many records with:
http://&lt;hostport&gt;/api/scan/&lt;dataset&gt;?records=N
</pre></div>
</div>
<p>Continuing the above example with data registered via HiveServer2, we should see:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Read the entire dataset</span>
<span class="n">curl</span> <span class="o">&lt;</span><span class="n">hostport</span><span class="o">&gt;/</span><span class="n">api</span><span class="o">/</span><span class="n">scan</span><span class="o">/</span><span class="n">sample</span>

<span class="c1"># If running on a kerberized cluster, you will need to authenticate the curl</span>
<span class="c1"># request. This assumes you have local kerberos credentials (i.e. already ran</span>
<span class="c1"># kinit)</span>
<span class="n">curl</span> <span class="o">--</span><span class="n">negotiate</span> <span class="o">-</span><span class="n">u</span> <span class="p">:</span> <span class="n">hostport</span><span class="o">/</span><span class="n">api</span><span class="o">/</span><span class="n">scan</span><span class="o">/</span><span class="n">sample</span>
</pre></div>
</div>
<p>Note that this API, like the other CDAS scan APIs, is intended to feed data into analytics tools.
The analytics tools perform any final computation. For example, an aggregate query like
“select count(*) from nytaxi.parquet_data” may return multiple rows with partial sums.
Here, the client (pandas/R/presto etc.) would perform the final computation and return a single row.
Other aggregate functions like min, max and sum are not supported.</p>
</div>
<div class="section" id="python-pandas-integration">
<span id="python-pandas-integration"></span><h3>Python Pandas Integration<a class="headerlink" href="#python-pandas-integration" title="Permalink to this headline">¶</a></h3>
<p>Reading the data into a panda data frame is very simple with the REST API.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_json</span><span class="p">(</span><span class="s1">&#39;http://&lt;hostport&gt;/api/scan/&lt;dataset&gt;&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="advance-configurations">
<span id="advance-configurations"></span><h2>Advance configurations<a class="headerlink" href="#advance-configurations" title="Permalink to this headline">¶</a></h2>
<div class="section" id="network-related-configs">
<span id="network-related-configs"></span><h3>Network related configs<a class="headerlink" href="#network-related-configs" title="Permalink to this headline">¶</a></h3>
<p>This configs are often not required and the defaults should suffice. These can be
adjusted if the the client observes timeout behavior.</p>
<p><strong>recordservice.planner.retry.attempts</strong></p>
<p>Optional configuration for the maximum number of attempts to retry RPCs with planner.</p>
<p><strong>recordservice.worker.retry.attempts</strong></p>
<p>Optional configuration for the maximum number of attempts to retry RPCs with worker.</p>
<p><strong>recordservice.planner.retry.sleepMs</strong></p>
<p>Optional configuration for sleep between retry attempts with planner.</p>
<p><strong>recordservice.worker.retry.sleepMs</strong></p>
<p>Optional configuration for sleep between retry attempts with worker.</p>
<p><strong>recordservice.planner.connection.timeoutMs</strong></p>
<p>Optional configuration for timeout when initially connecting to the planner service.</p>
<p><strong>recordservice.worker.connection.timeoutMs</strong></p>
<p>Optional configuration for timeout when initially connecting to the worker service.</p>
<p><strong>recordservice.planner.rpc.timeoutMs</strong></p>
<p>Optional configuration for timeout for planner RPCs (after connection is established).</p>
<p><strong>recordservice.worker.rpc.timeoutMs</strong></p>
<p>Optional configuration for timeout for worker RPCs (after connection ie established).</p>
</div>
<div class="section" id="performance-related-configs">
<span id="performance-related-configs"></span><h3>Performance related configs<a class="headerlink" href="#performance-related-configs" title="Permalink to this headline">¶</a></h3>
<p>These settings can fine tune the performance behavior. It is generally not needed to set
these as the server will compute a good value automatically.</p>
<p><strong>recordservice.task.fetch.size</strong></p>
<p>Optional configuration option for performance tuning that configures the max number of
records returned when fetching results from the workers.</p>
<p><strong>recordservice.task.plan.maxTasks</strong>
Optional configuration for the hinted maximum number of tasks to generate per PlanRequest.</p>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Cerebro Data Access Service Integration</a><ul>
<li><a class="reference internal" href="#hadoop-ecosystem-tools-spark-mapreduce-hive-presto">Hadoop ecosystem tools (spark, mapreduce, hive, presto)</a><ul>
<li><a class="reference internal" href="#client-libraries">Client libraries</a></li>
<li><a class="reference internal" href="#configurations">Configurations</a><ul>
<li><a class="reference internal" href="#required">Required</a></li>
<li><a class="reference internal" href="#authentication-configs">Authentication configs</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#tool-integration">Tool integration</a><ul>
<li><a class="reference internal" href="#hiveserver2-beeline-hive-cli">HiveServer2/Beeline/hive cli</a><ul>
<li><a class="reference internal" href="#setting-up-the-admin-role-quick-start">Setting up the admin role quick start</a></li>
<li><a class="reference internal" href="#creating-a-dataset">Creating a dataset</a></li>
<li><a class="reference internal" href="#creating-a-view-and-granting-access-to-another-role">Creating a view and granting access to another role</a></li>
</ul>
</li>
<li><a class="reference internal" href="#mapreduce-integration">MapReduce Integration</a><ul>
<li><a class="reference internal" href="#configuration">Configuration</a></li>
<li><a class="reference internal" href="#dependency-management">Dependency management</a></li>
</ul>
</li>
<li><a class="reference internal" href="#pig-integration">Pig Integration</a><ul>
<li><a class="reference internal" href="#configuration">Configuration</a></li>
</ul>
</li>
<li><a class="reference internal" href="#hue-integration">Hue Integration</a><ul>
<li><a class="reference internal" href="#troubleshooting">Troubleshooting</a></li>
</ul>
</li>
<li><a class="reference internal" href="#impala-integration">Impala Integration</a></li>
<li><a class="reference internal" href="#spark-integration">Spark Integration</a><ul>
<li><a class="reference internal" href="#hivecontext">HiveContext</a></li>
<li><a class="reference internal" href="#configuration">Configuration</a></li>
</ul>
</li>
<li><a class="reference internal" href="#rest-scan-api">REST Scan API</a></li>
<li><a class="reference internal" href="#python-pandas-integration">Python Pandas Integration</a></li>
</ul>
</li>
<li><a class="reference internal" href="#advance-configurations">Advance configurations</a><ul>
<li><a class="reference internal" href="#network-related-configs">Network related configs</a></li>
<li><a class="reference internal" href="#performance-related-configs">Performance related configs</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/ClientIntegration.md.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2018, azzy.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.7.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
      |
      <a href="_sources/ClientIntegration.md.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>